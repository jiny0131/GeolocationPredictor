{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75220071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load files\n",
    "import csv\n",
    "def open_file(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        label = []\n",
    "        feature = []\n",
    "        for i in reader:\n",
    "            label.append(i[0])\n",
    "            feature.append(i[2])\n",
    "        return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04004d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "glove_train_feature, glove_train_label = open_file('D:/comp90049/project3/data/train_glove300.csv')\n",
    "glove_dev_feature, glove_dev_label = open_file('D:/comp90049/project3/data/dev_glove300.csv')\n",
    "glove_test_feature, glove_test_label = open_file('D:/comp90049/project3/data/test_glove300.csv')\n",
    "# tfidf\n",
    "tfidf_train_feature, tfidf_train_label = open_file('D:/comp90049/project3/data/train_tfidf.csv')\n",
    "tfidf_dev_feature, tfidf_dev_label = open_file('D:/comp90049/project3/data/dev_tfidf.csv')\n",
    "tfidf_test_feature, tfidf_test_label = open_file('D:/comp90049/project3/data/test_tfidf.csv')\n",
    "# node2vec\n",
    "node2vec_train_feature, node2vec_train_label = open_file('D:/comp90049/project3/data/train_node2vec300.csv')\n",
    "node2vec_dev_feature, node2vec_dev_label = open_file('D:/comp90049/project3/data/dev_node2vec300.csv')\n",
    "node2vec_test_feature, node2vec_test_label = open_file('D:/comp90049/project3/data/test_node2vec300.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a06ef983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def standard_label(label):\n",
    "    dict = {\"MIDWEST\" : 0, \"NORTHEAST\" : 1, \"SOUTH\" : 2, \"WEST\" : 3}\n",
    "    standard_label = []\n",
    "    for l in label:\n",
    "        standard_label.append(dict[l])\n",
    "    return np.array(standard_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9d874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_glove_train_label = standard_label(glove_train_label)\n",
    "standard_glove_dev_label = standard_label(glove_dev_label)\n",
    "standard_node2vec_train_label = standard_label(node2vec_train_label)\n",
    "standard_node2vec_dev_label = standard_label(node2vec_dev_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4919180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def standard_glove_feature(feature):\n",
    "    pos = 0\n",
    "    standard_glove_feature = []\n",
    "    for pos in range(len(feature)):\n",
    "        cur = list(map(float, feature[pos].split()))\n",
    "        standard_glove_feature.append(cur)\n",
    "    return np.array(standard_glove_feature)\n",
    "\n",
    "def standard_tfidf_feature(feature):\n",
    "    standard_tfidf_feature = []\n",
    "    for pos in range(len(feature)):\n",
    "        feature[pos] = ast.literal_eval(feature[pos])\n",
    "    for f in feature:\n",
    "        index_dict = {}\n",
    "        for i in f:\n",
    "            index_dict[i[0]] = i[1]\n",
    "        one_tfidf = []\n",
    "        for count in range(2037):\n",
    "            if(count not in index_dict):\n",
    "                one_tfidf.append(0)\n",
    "            else:\n",
    "                one_tfidf.append(index_dict[count])\n",
    "        standard_tfidf_feature.append(one_tfidf)\n",
    "    return np.array(standard_tfidf_feature)\n",
    "\n",
    "def standard_node2vec_feature(feature):\n",
    "    pos = 0\n",
    "    standard_node2vec_feature = []\n",
    "    for pos in range(len(feature)):\n",
    "        ns = feature[pos][1:-1].replace('\\n','')\n",
    "        cur = list(map(float, ns.split()))\n",
    "        standard_node2vec_feature.append(cur)\n",
    "    return np.array(standard_node2vec_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee4687eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def open_file(filename):\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        label = {}\n",
    "        feature = {}\n",
    "        for i in reader:\n",
    "            if(i[1] not in feature):\n",
    "                feature[i[1]] = i[2]\n",
    "                label[i[1]] = i[0]\n",
    "            else:\n",
    "                feature[i[1]] = feature[i[1]] + i[2]\n",
    "        return feature, label\n",
    "    \n",
    "full_train_feature, full_train_label = open_file('D:/comp90049/project3/data/train_full.csv')\n",
    "full_dev_feature, full_dev_label = open_file('D:/comp90049/project3/data/dev_full.csv')\n",
    "full_test_feature, full_test_label = open_file('D:/comp90049/project3/data/test_full.csv')\n",
    "def new_feature_label(ofeature, olabel):\n",
    "    nfeature = []\n",
    "    nlabel = []\n",
    "    for user in ofeature:\n",
    "        nfeature.append(ofeature[user])\n",
    "        nlabel.append(olabel[user])\n",
    "    return nfeature, nlabel\n",
    "n_train_feature, n_train_label = new_feature_label(full_train_feature, full_train_label)\n",
    "n_dev_feature, n_dev_label = new_feature_label(full_dev_feature, full_dev_label)\n",
    "n_test_feature, n_test_label = new_feature_label(full_test_feature, full_test_label)\n",
    "\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "        sublinear_tf=True,\n",
    "        strip_accents='unicode',\n",
    "        analyzer='word',\n",
    "        stop_words='english',\n",
    "        min_df = 300)\n",
    "word_vectorizer.fit(n_train_feature)\n",
    "f_tfidf_train_feature = word_vectorizer.transform(n_train_feature).toarray()\n",
    "f_tfidf_dev_feature = word_vectorizer.transform(n_dev_feature).toarray()\n",
    "f_tfidf_test_feature = word_vectorizer.transform(n_test_feature).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0e674a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n",
      "315\n"
     ]
    }
   ],
   "source": [
    "print(len(f_tfidf_train_feature[0]))\n",
    "print(len(f_tfidf_test_feature[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9faabb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove\n",
    "glove_train_feature = standard_glove_feature(glove_train_feature)\n",
    "glove_dev_feature = standard_glove_feature(glove_dev_feature)\n",
    "glove_test_feature = standard_glove_feature(glove_test_feature)\n",
    "# tf-idf\n",
    "tfidf_train_feature = standard_tfidf_feature(tfidf_train_feature)\n",
    "tfidf_dev_feature = standard_tfidf_feature(tfidf_dev_feature)\n",
    "tfidf_test_feature = standard_tfidf_feature(tfidf_test_feature)\n",
    "# node2vec\n",
    "node2vec_train_feature = standard_node2vec_feature(node2vec_train_feature)\n",
    "node2vec_dev_feature = standard_node2vec_feature(node2vec_dev_feature)\n",
    "node2vec_test_feature = standard_node2vec_feature(node2vec_test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee868194",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras import regularizers\n",
    "from keras.utils.np_utils import to_categorical\n",
    "# It is an original version of MENET model which just have 39% accuracy with the development dataset.\n",
    "\n",
    "# # node2vec part\n",
    "# x = Input(shape = (300,))\n",
    "# y = Dense(150)(x)\n",
    "# node2vec = Model(inputs = x, outputs = y)\n",
    "\n",
    "\n",
    "# # tf-idf part\n",
    "# x = Input(shape = (315,))\n",
    "# y = Dense(150)(x)\n",
    "# tfidf = Model(inputs = x, outputs = y)\n",
    "\n",
    "\n",
    "# # glove 300 part\n",
    "# x = Input(shape = (300,))\n",
    "# y = Dense(30)(x)\n",
    "# glove = Model(inputs = x, outputs = y)\n",
    "\n",
    "\n",
    "\n",
    "# # MENET\n",
    "# combined = concatenate([node2vec.output, tfidf.output, glove.output])\n",
    "# relu = Dense(330, activation = \"relu\")(combined)\n",
    "# result = Dense(4, activation = \"softmax\", kernel_regularizer = regularizers.l2(0.1))(relu)\n",
    "# model = Model(inputs = [node2vec.input, tfidf.input, glove.input], outputs = result)\n",
    "\n",
    "\n",
    "# opt = SGD(lr=1e-4, decay=1e-4 / 200)\n",
    "# model.compile(loss = categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "# cb = EarlyStopping(monitor='val_loss', patience=4,verbose=1)\n",
    "# model.fit([node2vec_train_feature, tfidf_train_feature, glove_train_feature], to_categorical(standard_glove_train_label, num_classes = 4),\\\n",
    "#           validation_data=([node2vec_dev_feature, tfidf_dev_feature, glove_dev_feature], to_categorical(standard_glove_dev_label, num_classes = 4)),\\\n",
    "#           callbacks = [cb], epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6ffd11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3400 samples, validate on 300 samples\n",
      "Epoch 1/200\n",
      "3400/3400 [==============================] - 0s 56us/step - loss: 1.9508 - accuracy: 0.4806 - val_loss: 1.9668 - val_accuracy: 0.3867\n",
      "Epoch 2/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 1.6281 - accuracy: 0.5988 - val_loss: 1.8441 - val_accuracy: 0.3967\n",
      "Epoch 3/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 1.4245 - accuracy: 0.6397 - val_loss: 1.7473 - val_accuracy: 0.4533\n",
      "Epoch 4/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 1.2771 - accuracy: 0.6844 - val_loss: 1.6685 - val_accuracy: 0.4567\n",
      "Epoch 5/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 1.1663 - accuracy: 0.7024 - val_loss: 1.6072 - val_accuracy: 0.4500\n",
      "Epoch 6/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 1.0801 - accuracy: 0.7138 - val_loss: 1.5508 - val_accuracy: 0.4533\n",
      "Epoch 7/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 1.0114 - accuracy: 0.7232 - val_loss: 1.5078 - val_accuracy: 0.4533\n",
      "Epoch 8/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.9557 - accuracy: 0.7324 - val_loss: 1.4661 - val_accuracy: 0.4667\n",
      "Epoch 9/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.9116 - accuracy: 0.7403 - val_loss: 1.4351 - val_accuracy: 0.4800\n",
      "Epoch 10/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.8734 - accuracy: 0.7435 - val_loss: 1.4121 - val_accuracy: 0.4600\n",
      "Epoch 11/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.8405 - accuracy: 0.7476 - val_loss: 1.3916 - val_accuracy: 0.4633\n",
      "Epoch 12/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.8149 - accuracy: 0.7518 - val_loss: 1.3776 - val_accuracy: 0.4600\n",
      "Epoch 13/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.7915 - accuracy: 0.7568 - val_loss: 1.3607 - val_accuracy: 0.4600\n",
      "Epoch 14/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.7711 - accuracy: 0.7594 - val_loss: 1.3450 - val_accuracy: 0.4633\n",
      "Epoch 15/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.7531 - accuracy: 0.7612 - val_loss: 1.3389 - val_accuracy: 0.4633\n",
      "Epoch 16/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.7375 - accuracy: 0.7609 - val_loss: 1.3355 - val_accuracy: 0.4767\n",
      "Epoch 17/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.7236 - accuracy: 0.7641 - val_loss: 1.3196 - val_accuracy: 0.4767\n",
      "Epoch 18/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.7103 - accuracy: 0.7726 - val_loss: 1.3152 - val_accuracy: 0.4900\n",
      "Epoch 19/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.6988 - accuracy: 0.7706 - val_loss: 1.3165 - val_accuracy: 0.4700\n",
      "Epoch 20/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.6869 - accuracy: 0.7779 - val_loss: 1.3114 - val_accuracy: 0.4767\n",
      "Epoch 21/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.6775 - accuracy: 0.7815 - val_loss: 1.3045 - val_accuracy: 0.4867\n",
      "Epoch 22/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.6681 - accuracy: 0.7835 - val_loss: 1.3061 - val_accuracy: 0.4933\n",
      "Epoch 23/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.6586 - accuracy: 0.7853 - val_loss: 1.3065 - val_accuracy: 0.4767\n",
      "Epoch 24/200\n",
      "3400/3400 [==============================] - 0s 32us/step - loss: 0.6515 - accuracy: 0.7882 - val_loss: 1.3089 - val_accuracy: 0.4933\n",
      "Epoch 25/200\n",
      "3400/3400 [==============================] - 0s 31us/step - loss: 0.6442 - accuracy: 0.7912 - val_loss: 1.3065 - val_accuracy: 0.4900\n",
      "Epoch 00025: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1bb2f71c208>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node2vec part\n",
    "x = Input(shape = (300,))\n",
    "y = Dense(150)(x)\n",
    "node2vec = Model(inputs = x, outputs = y)\n",
    "\n",
    "# tf-idf part\n",
    "x = Input(shape = (315,))\n",
    "y = Dense(150)(x)\n",
    "tfidf = Model(inputs = x, outputs = y)\n",
    "\n",
    "# MENET\n",
    "combined = concatenate([node2vec.output, tfidf.output])\n",
    "relu = Dense(300, activation = \"relu\")(combined)\n",
    "result = Dense(4, activation = \"softmax\", kernel_regularizer = regularizers.l2(0.1))(relu)\n",
    "i_model = Model(inputs = [node2vec.input, tfidf.input], outputs = result)\n",
    "\n",
    "\n",
    "opt = Adam(lr=1e-4, decay=1e-4 / 200)\n",
    "i_model.compile(loss = categorical_crossentropy,optimizer=opt,metrics=['accuracy'])\n",
    "cb = EarlyStopping(monitor='val_loss', patience=4,verbose=1)\n",
    "i_model.fit([node2vec_train_feature, f_tfidf_train_feature], to_categorical(standard_node2vec_train_label, num_classes = 4),\\\n",
    "          validation_data=([node2vec_dev_feature, f_tfidf_dev_feature], to_categorical(standard_node2vec_dev_label, num_classes = 4)),\\\n",
    "          callbacks = [cb], epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa436f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46352941176470586\n",
      "0.49054897417691246\n"
     ]
    }
   ],
   "source": [
    "# Multinomial naive bayes classifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(tfidf_train_feature, tfidf_train_label)\n",
    "predict = clf.predict(tfidf_dev_feature)\n",
    "print(accuracy(predict, tfidf_dev_label)) \n",
    "opredict = clf.predict(tfidf_train_feature)\n",
    "print(accuracy(opredict, tfidf_train_label)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "c8db2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def out_file(filename, data):\n",
    "    out_f = open(\"predict_f.csv\", 'w', newline='')\n",
    "    writer = csv.writer(out_f)\n",
    "    head = [\"id\", \"region\"]\n",
    "    writer.writerow(head)\n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        user = []\n",
    "        count = -1\n",
    "        norm_count = 1\n",
    "        for i in reader:\n",
    "            if(i[1] not in user):\n",
    "                count += 1\n",
    "                user.append(i[1])\n",
    "                writer.writerow((norm_count, data[count]))\n",
    "            else:\n",
    "                writer.writerow((norm_count, data[count]))\n",
    "            norm_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "id": "15dcf709",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_result(model.predict([node2vec_test_feature, f_tfidf_test_feature]))\n",
    "\n",
    "r_dict = {0: \"MIDWEST\", 1: \"NORTHEAST\", 2: \"SOUTH\", 3 : \"WEST\"}\n",
    "text_result = []\n",
    "for r in result:\n",
    "    text_result.append(r_dict[r])\n",
    "out_file('D:/comp90049/project3/data/test_full.csv', text_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b851c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def accuracy(predict, gold):\n",
    "    acc = 0\n",
    "    for pos in range(len(predict)):\n",
    "        if(predict[pos] ==  gold[pos]):\n",
    "            acc += 1\n",
    "    accuracy = acc / len(predict)\n",
    "    return accuracy\n",
    "def zero_r_prediction(label):\n",
    "    label_counter = Counter(label)\n",
    "    zero_r_prediction = []\n",
    "    majority_class = str(label_counter.most_common(1)[0][0])\n",
    "    for i in range(len(label)):\n",
    "        zero_r_prediction.append(majority_class)\n",
    "    return zero_r_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bec576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39300422287828396\n"
     ]
    }
   ],
   "source": [
    "zero_r = zero_r_prediction(glove_train_label)\n",
    "print(accuracy(zero_r, glove_train_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7dc6708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.37429193899782137\n"
     ]
    }
   ],
   "source": [
    "zero_r = zero_r_prediction(glove_dev_label)\n",
    "print(accuracy(zero_r, glove_dev_label))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
